{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BIGNETWORK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\BIGNETWORK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import emoji\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alg_telcom_2_2k.xlsx',\n",
       " 'facebook (1).xlsx',\n",
       " 'facebook (2).xlsx',\n",
       " 'instagram (1).xlsx',\n",
       " 'instagram (2).xlsx',\n",
       " 'instagram (3).xlsx',\n",
       " 'instagram (4).xlsx',\n",
       " 'instagram (5).xlsx',\n",
       " 'instagram (6).xlsx',\n",
       " 'instagram (7).xlsx',\n",
       " 'instagram (8).xlsx',\n",
       " 'instagram.xlsx']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = ('C:/Users/BIGNETWORK/Desktop/PFE/dataset/resaux scraper')\n",
    "files = os.listdir(path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alg_telcom_2_2k.xlsx',\n",
       " 'facebook (1).xlsx',\n",
       " 'facebook (2).xlsx',\n",
       " 'instagram (1).xlsx',\n",
       " 'instagram (2).xlsx',\n",
       " 'instagram (3).xlsx',\n",
       " 'instagram (4).xlsx',\n",
       " 'instagram (5).xlsx',\n",
       " 'instagram (6).xlsx',\n",
       " 'instagram (7).xlsx',\n",
       " 'instagram (8).xlsx',\n",
       " 'instagram.xlsx']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_xls = [f for f in files if f[-4:] == 'xlsx']\n",
    "files_xls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for f in files_xls:\n",
    "    data = pd.read_excel(f'C:/Users/BIGNETWORK/Desktop/PFE/dataset/resaux scraper/{f}')\n",
    "    data.drop(data.index[0:0],inplace=True)\n",
    "    df = df.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comments  class\n",
      "0    باينة كتبتوها نهار السبت حتى اليوم باش تبارتاج...    NaN\n",
      "1         لوكان ستنيتو حتى نهار الخميس خير و درتو بيان    NaN\n",
      "2                       الزبون لا دخل له في البكالوريا    NaN\n",
      "3    شركة.... تعجز عن وضع أجهزة تشويش بمراكز الإمتح...    NaN\n",
      "4    دوماج عندي لافامي في الكونت هذا كون عبرتلكم عن...    NaN\n",
      "..                                                 ...    ...\n",
      "785                                                  🔥    NaN\n",
      "786                                               ❤️❤️    NaN\n",
      "787                                               ❤️❤️    NaN\n",
      "788                         NChllh  b rebi ping yhbt 😂    NaN\n",
      "789                                              ❤️❤️😍    NaN\n",
      "\n",
      "[6834 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"resaux_comments.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0=pd.read_excel(\"C:/Users/BIGNETWORK/Desktop/PFE/dataset/project/git-one/datasets/resaux_comments.xlsx\")\n",
    "df1=pd.read_excel(\"C:/Users/BIGNETWORK/Desktop/PFE/dataset/project/git-one/datasets/hateonly_mehdi.xlsx\")\n",
    "df2=pd.read_excel(\"C:/Users/BIGNETWORK/Desktop/PFE/dataset/project/git-one/datasets/fb_comments.xlsx\")\n",
    "df3=pd.read_excel(\"C:/Users/BIGNETWORK/Desktop/PFE/dataset/project/git-one/datasets/hainer_ranim.xlsx\")\n",
    "df4=pd.read_excel(\"C:/Users/BIGNETWORK/Desktop/PFE/dataset/project/git-one/datasets/neutre_mehdi.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df0, df1, df2, df3,df4], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>class</th>\n",
       "      <th>comments_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tagdo3o cnx men lhad hta lkhmis wzid jam3a mkn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tagdo3o cnx men lhad hta lkhmis wzid jam3a mkn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>و كونيكسيون ديالها رهج و زيرو كارتة و هدا وين ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>كونيكسيون ديالها رهج زيرو كارتة هدا عرفت 60 gb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sera9in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sera9in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>هذي الكفار او ماداروهاش</td>\n",
       "      <td>NaN</td>\n",
       "      <td>الكفار ماداروهاش</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>تافهين   وخدماتكم دون المستوى  وماشكيتش تتحسنو</td>\n",
       "      <td>NaN</td>\n",
       "      <td>تافهين وخدماتكم المستوى وماشكيتش تحسنو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10076</th>\n",
       "      <td>تحيا جيزي راهي علامة الشبكة 100/100 متسرقش نكم...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>تحيا جيزي راهي علامة الشبكة 10 10 متسرقش نكمل ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10077</th>\n",
       "      <td>انا مكان مادخلني فالباك راني نخلص فيها  وهذا حقي</td>\n",
       "      <td>NaN</td>\n",
       "      <td>مادخلني فالباك راني نخلص وهذا حقي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10078</th>\n",
       "      <td>iOS !!!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10079</th>\n",
       "      <td>الله يالكونكسين تاعكم رجعت تعيف تعيف تعييييف</td>\n",
       "      <td>1.0</td>\n",
       "      <td>اله يالكونكسين تاعكم رجعت تعيف تعيف تعيف</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10080</th>\n",
       "      <td>كل شيء ثقيل عندكم حتى الاشعارات</td>\n",
       "      <td>NaN</td>\n",
       "      <td>شيء ثقيل الاشعارات</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9840 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comments  class  \\\n",
       "0      Tagdo3o cnx men lhad hta lkhmis wzid jam3a mkn...    NaN   \n",
       "1      و كونيكسيون ديالها رهج و زيرو كارتة و هدا وين ...    NaN   \n",
       "2                                                Sera9in    NaN   \n",
       "3                                هذي الكفار او ماداروهاش    NaN   \n",
       "4         تافهين   وخدماتكم دون المستوى  وماشكيتش تتحسنو    NaN   \n",
       "...                                                  ...    ...   \n",
       "10076  تحيا جيزي راهي علامة الشبكة 100/100 متسرقش نكم...    NaN   \n",
       "10077   انا مكان مادخلني فالباك راني نخلص فيها  وهذا حقي    NaN   \n",
       "10078                                            iOS !!!    NaN   \n",
       "10079       الله يالكونكسين تاعكم رجعت تعيف تعيف تعييييف    1.0   \n",
       "10080                    كل شيء ثقيل عندكم حتى الاشعارات    NaN   \n",
       "\n",
       "                                          comments_clean  \n",
       "0      Tagdo3o cnx men lhad hta lkhmis wzid jam3a mkn...  \n",
       "1      كونيكسيون ديالها رهج زيرو كارتة هدا عرفت 60 gb...  \n",
       "2                                                Sera9in  \n",
       "3                                       الكفار ماداروهاش  \n",
       "4                 تافهين وخدماتكم المستوى وماشكيتش تحسنو  \n",
       "...                                                  ...  \n",
       "10076  تحيا جيزي راهي علامة الشبكة 10 10 متسرقش نكمل ...  \n",
       "10077                  مادخلني فالباك راني نخلص وهذا حقي  \n",
       "10078                                               iOS   \n",
       "10079           اله يالكونكسين تاعكم رجعت تعيف تعيف تعيف  \n",
       "10080                                 شيء ثقيل الاشعارات  \n",
       "\n",
       "[9840 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"C:/Users/BIGNETWORK/Desktop/PFE/dataset/dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(subset = ['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments'].isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "---\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family:Georgia'> Preprocessing of Text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family:serif'> Function to Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_emoji(text):\n",
    "    return emoji.replace_emoji(text,replace=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family:serif'> removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loukan temdou alef jiga batel tqa3dou halazouuun .\n"
     ]
    }
   ],
   "source": [
    "# Load stopwords for each language\n",
    "with open('C:/Users/BIGNETWORK/Desktop/PFE/dataset/project/git-one/Algerian-Arabic-stop-words.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words_ar_dz = set([line.strip() for line in f])\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words_fr = set(stopwords.words('french'))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Create a custom tokenizer for Arabic words\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "# Define a function to remove stop words from a text\n",
    "def remove_stopwords(text):\n",
    "    words = tokenizer.tokenize(text)\n",
    "    words_filtered = []\n",
    "    for word in words:\n",
    "        if word not in stop_words_ar_dz and word not in stop_words_en and word not in stop_words_fr:\n",
    "            words_filtered.append(word)\n",
    "    return ' '.join(words_filtered)\n",
    "tet = \"Wlh loukan temdou alef jiga batel tqa3dou halazouuun.\"\n",
    "tet = remove_stopwords(tet)\n",
    "print(tet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family:serif'> Fucntion to remove special characters, URLs, duplicated letters, punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Apply the clean_text function to the desired column(s) in the DataFrame\\nif df['comments'].dtype == 'object':\\n    df['comments_clean'] = df['comments'].apply(clean_text)\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to perform the text cleaning operations\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation marks \n",
    "    text = re.sub(r'[@#&$]\\w+', '', text)  # Remove special characters\n",
    "    #text = re.sub(r'\\d+', '', text)  # Remove numbers \n",
    "    text = re.sub(r'(\\w)\\1+', r'\\1', text) # remove duplicated letters\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    return text\n",
    "\n",
    "'''# Apply the clean_text function to the desired column(s) in the DataFrame\n",
    "if df['comments'].dtype == 'object':\n",
    "    df['comments_clean'] = df['comments'].apply(clean_text)'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family:serif'> Function to remove mutiple sequence spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mult_spaces(text):\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family:serif'> Function to Preprocess the text by applying all above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = strip_emoji(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = clean_text(text)\n",
    "    text = remove_mult_spaces(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIGNETWORK\\AppData\\Local\\Temp\\ipykernel_13008\\2574735620.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['comments_clean'] = df['comments'].apply(preprocess)\n"
     ]
    }
   ],
   "source": [
    "if df['comments'].dtype == 'object':\n",
    "    df['comments_clean'] = df['comments'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>class</th>\n",
       "      <th>comments_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>سكت دهراً .. ونطق كفراً</td>\n",
       "      <td>NaN</td>\n",
       "      <td>سكت دهرا ونطق كفرا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>سراق يفليكسي لواحد يجي ياكتيفي يلقاها راحت درا...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>سراق يفليكسي لواحد يجي ياكتيفي يلقاها راحت درا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9875</th>\n",
       "      <td>,ن. م بإلحاح و بكل روح وطنية ن.م ن. م ن. م</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ن م بإلحاح بكل روح وطنية م م م</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6274</th>\n",
       "      <td>خلي راني صايم مور فطور نشا الله نتهلا</td>\n",
       "      <td>NaN</td>\n",
       "      <td>خلي راني صايم مور فطور نشا اله نتهلا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4343</th>\n",
       "      <td>هذي طلقتوها يامات لولة تع باك هذا وين فاتت pub...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>طلقتوها يامات لولة باك فات publication تعكم شر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5161</th>\n",
       "      <td>نخلة هذيك تاع شطيح ورطيح هههههههه خاطي قاع بوب...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>نخلة هذيك شطيح ورطيح ه خاطي قاع بوبجي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4017</th>\n",
       "      <td>Nik ymah l9hba li b9a ychri djezzy ynik mo ykh...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Nik ymah l9hba li b9a ychri djezy ynik mo ykhl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>Ani bgratwi yzbi flixsit dawhomli  djezzy ta3 zbi</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Ani bgratwi yzbi flixsit dawhomli djezy ta3 zbi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>Dirolna free fire 😭</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dirolna fre fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>صحا ليندينlinkdedln  ثاني يتريشو بيه</td>\n",
       "      <td>NaN</td>\n",
       "      <td>صحا ليندينlinkdedln ثاني يتريشو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6837</th>\n",
       "      <td>دجيزي أسوأ شبكة</td>\n",
       "      <td>NaN</td>\n",
       "      <td>دجيزي أسوأ شبكة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>وانا وقتاش تريبونديو على رسالتي لي في الخاص /:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>وانا وقتاش تريبونديو رسالتي الخاص</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10039</th>\n",
       "      <td>شكون قالكم رانا صابرين</td>\n",
       "      <td>NaN</td>\n",
       "      <td>شكون قالكم صابرين</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>اكيد ما حبش يرجع الأموال المنهوبة التي ينتظرها...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>اكيد حبش يرجع الأموال المنهوبة ينتظرها بوصبع ل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885</th>\n",
       "      <td>شحال خلصوك موبليس</td>\n",
       "      <td>NaN</td>\n",
       "      <td>شحال خلصوك موبليس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>اللهم اجعل عيشتنا في هذه البلاد تخفيف ذنوب</td>\n",
       "      <td>NaN</td>\n",
       "      <td>الهم اجعل عيشتنا البلاد تخفيف ذنوب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7406</th>\n",
       "      <td>ونديرها باه نلعب تاع دينصور هديك ولا كيفاه نتو...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ونديرها باه نلعب دينصور نتوم وقتاش تحسنو الخدم...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8948</th>\n",
       "      <td>Thabou tsem3ouhom ya rkhas ah</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thabou tsem3ouhom ya rkhas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7571</th>\n",
       "      <td>قدمت شكوى على سوء الخدمة وضعف الشبكة فلم نتلقى...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>قدمت شكوى سوء الخدمة وضعف الشبكة فلم نتلقى الا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6955</th>\n",
       "      <td>المغرب والله ما فايق ليك من النعاس اسي تبون ال...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>المغرب واله فايق النعاس اسي تبون العام جا يلقا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>وعلاش كونكسيو تاعكم تقيل تعايو راهج</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>وعلاش كونكسيو تاعكم تقيل تعايو راهج</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>راك ثقيلة جيزي نشاله تولدي بأقرب وقت🙂</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ثقيلة جيزي نشاله تولدي بأقرب وقت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7060</th>\n",
       "      <td>بدل الشعار ورده 50جيجا طونيكتي وحدك ومنع أصدقا...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>بدل الشعار ورده 50جيجا طونيكتي وحدك ومنع أصدقا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9184</th>\n",
       "      <td>اين ريزو انا لا اراه يااو لكونكسيو راحت يا جيز...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>اين ريزو اراه ياو لكونكسيو راحت جيزي خويا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5982</th>\n",
       "      <td>🐌🐌🐌🐌🐌🐌🐌🐌</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comments  class  \\\n",
       "4042                             سكت دهراً .. ونطق كفراً    NaN   \n",
       "796    سراق يفليكسي لواحد يجي ياكتيفي يلقاها راحت درا...    NaN   \n",
       "9875          ,ن. م بإلحاح و بكل روح وطنية ن.م ن. م ن. م    NaN   \n",
       "6274             خلي راني صايم مور فطور نشا الله نتهلا      NaN   \n",
       "4343   هذي طلقتوها يامات لولة تع باك هذا وين فاتت pub...    NaN   \n",
       "5161   نخلة هذيك تاع شطيح ورطيح هههههههه خاطي قاع بوب...    NaN   \n",
       "4017   Nik ymah l9hba li b9a ychri djezzy ynik mo ykh...   -1.0   \n",
       "2041   Ani bgratwi yzbi flixsit dawhomli  djezzy ta3 zbi   -1.0   \n",
       "4169                                 Dirolna free fire 😭    NaN   \n",
       "2556                صحا ليندينlinkdedln  ثاني يتريشو بيه    NaN   \n",
       "6837                                     دجيزي أسوأ شبكة    NaN   \n",
       "641       وانا وقتاش تريبونديو على رسالتي لي في الخاص /:    NaN   \n",
       "10039                             شكون قالكم رانا صابرين    NaN   \n",
       "6045   اكيد ما حبش يرجع الأموال المنهوبة التي ينتظرها...    1.0   \n",
       "4885                                   شحال خلصوك موبليس    NaN   \n",
       "6816          اللهم اجعل عيشتنا في هذه البلاد تخفيف ذنوب    NaN   \n",
       "7406   ونديرها باه نلعب تاع دينصور هديك ولا كيفاه نتو...    NaN   \n",
       "8948                       Thabou tsem3ouhom ya rkhas ah    NaN   \n",
       "7571   قدمت شكوى على سوء الخدمة وضعف الشبكة فلم نتلقى...    NaN   \n",
       "6955   المغرب والله ما فايق ليك من النعاس اسي تبون ال...    1.0   \n",
       "825                  وعلاش كونكسيو تاعكم تقيل تعايو راهج   -1.0   \n",
       "2612               راك ثقيلة جيزي نشاله تولدي بأقرب وقت🙂    NaN   \n",
       "7060   بدل الشعار ورده 50جيجا طونيكتي وحدك ومنع أصدقا...    NaN   \n",
       "9184   اين ريزو انا لا اراه يااو لكونكسيو راحت يا جيز...    NaN   \n",
       "5982                                            🐌🐌🐌🐌🐌🐌🐌🐌    NaN   \n",
       "\n",
       "                                          comments_clean  \n",
       "4042                                 سكت دهرا ونطق كفرا   \n",
       "796    سراق يفليكسي لواحد يجي ياكتيفي يلقاها راحت درا...  \n",
       "9875                      ن م بإلحاح بكل روح وطنية م م م  \n",
       "6274                خلي راني صايم مور فطور نشا اله نتهلا  \n",
       "4343   طلقتوها يامات لولة باك فات publication تعكم شر...  \n",
       "5161               نخلة هذيك شطيح ورطيح ه خاطي قاع بوبجي  \n",
       "4017   Nik ymah l9hba li b9a ychri djezy ynik mo ykhl...  \n",
       "2041     Ani bgratwi yzbi flixsit dawhomli djezy ta3 zbi  \n",
       "4169                                    Dirolna fre fire  \n",
       "2556                     صحا ليندينlinkdedln ثاني يتريشو  \n",
       "6837                                     دجيزي أسوأ شبكة  \n",
       "641                   وانا وقتاش تريبونديو رسالتي الخاص   \n",
       "10039                                  شكون قالكم صابرين  \n",
       "6045   اكيد حبش يرجع الأموال المنهوبة ينتظرها بوصبع ل...  \n",
       "4885                                   شحال خلصوك موبليس  \n",
       "6816                  الهم اجعل عيشتنا البلاد تخفيف ذنوب  \n",
       "7406   ونديرها باه نلعب دينصور نتوم وقتاش تحسنو الخدم...  \n",
       "8948                          Thabou tsem3ouhom ya rkhas  \n",
       "7571   قدمت شكوى سوء الخدمة وضعف الشبكة فلم نتلقى الا...  \n",
       "6955   المغرب واله فايق النعاس اسي تبون العام جا يلقا...  \n",
       "825                  وعلاش كونكسيو تاعكم تقيل تعايو راهج  \n",
       "2612                    ثقيلة جيزي نشاله تولدي بأقرب وقت  \n",
       "7060   بدل الشعار ورده 50جيجا طونيكتي وحدك ومنع أصدقا...  \n",
       "9184           اين ريزو اراه ياو لكونكسيو راحت جيزي خويا  \n",
       "5982                                                      "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments_clean'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIGNETWORK\\AppData\\Local\\Temp\\ipykernel_13008\\3493959318.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['comments_clean'].replace('', np.nan, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments_clean'].replace('', np.nan, inplace=True)\n",
    "missing_values = df[df['comments_clean'].isnull()]\n",
    "#df=df.dropna(subset = ['comments_clean'])\n",
    "len(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(subset = ['comments_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_det(texte):\n",
    "    langage = detect(texte)\n",
    "    if langage not in ['en','fr']:\n",
    "        langage = 'dz'\n",
    "    \n",
    "    return langage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    }
   ],
   "source": [
    "text ='3andkom cnx ma9wda bzfffffff ou rizo bal m nhki'\n",
    "print(lang_det(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFXLMRobertaForSequenceClassification were initialized from the model checkpoint at papluca/xlm-roberta-base-language-detection.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n"
     ]
    }
   ],
   "source": [
    "# Load the language detection pipeline\n",
    "lang_detect = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n",
    "\n",
    "# Text to classify\n",
    "text = \"Pourquoi vous limitez le débit ?.\"\n",
    "\n",
    "# Run the language detection pipeline\n",
    "result = lang_detect(text)\n",
    "\n",
    "# Print the detected language\n",
    "print(result[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar\n"
     ]
    }
   ],
   "source": [
    "# Text to classify\n",
    "text = \"ڨاع ماشي conection surtout les environ ta3 sedikia\"\n",
    "\n",
    "# Run the language detection pipeline\n",
    "result = lang_detect(text)\n",
    "\n",
    "# Print the detected language\n",
    "print(result[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "# Utilisation de la méthode SequenceMatcher pour calculer la similarité entre deux chaînes de caractères\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "def detecter_langage(texte):\n",
    "    # Détection de la langue avec langdetect\n",
    "    langage = detect(texte)\n",
    "    \n",
    "   #  Si le texte est similaire à l'anglais, au français ou à l'arabe, il est classé comme dialecte algérien\n",
    "    if langage in ['en', 'fr', 'ar']:\n",
    "        similarites = {\n",
    "            'en': similar(texte, 'english'),\n",
    "            'fr': similar(texte, 'français'),\n",
    "            'ar': similar(texte, 'العربية')\n",
    "        }\n",
    "        # Le texte est classé comme dialecte algérien s'il est plus similaire à une de ces langues qu'à l'autre\n",
    "        if max(similarites.values()) == similarites[langage] or langage =='so':\n",
    "            langage = 'dz'\n",
    "    \n",
    "    return langage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n"
     ]
    }
   ],
   "source": [
    "txt = \"Kanet 3endi beseh réseau fiha mekhsous bezaf f la willaya de Tizi ouzou apart le centre ville makanch réseau\"\n",
    "print(detecter_langage(txt))\n",
    "  #  df['len'] = df['comments_nett'].apply(detecter_langage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIGNETWORK\\AppData\\Local\\Temp\\ipykernel_13008\\2026574333.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.at[index, 'language'] = result[0]['label']\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    text = str(row['comments_clean'])\n",
    "    if text:\n",
    "        try:\n",
    "            result = lang_detect(text)\n",
    "            df.at[index, 'language'] = result[0]['label']\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
