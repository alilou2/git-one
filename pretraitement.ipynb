{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import emoji\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alg_telcom_2_2k.xlsx',\n",
       " 'facebook (1).xlsx',\n",
       " 'facebook (2).xlsx',\n",
       " 'instagram (1).xlsx',\n",
       " 'instagram (2).xlsx',\n",
       " 'instagram (3).xlsx',\n",
       " 'instagram (4).xlsx',\n",
       " 'instagram (5).xlsx',\n",
       " 'instagram (6).xlsx',\n",
       " 'instagram (7).xlsx',\n",
       " 'instagram (8).xlsx',\n",
       " 'instagram.xlsx']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = ('C:/Users/BIGNETWORK/Desktop/PFE/dataset/resaux scraper')\n",
    "files = os.listdir(path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alg_telcom_2_2k.xlsx',\n",
       " 'facebook (1).xlsx',\n",
       " 'facebook (2).xlsx',\n",
       " 'instagram (1).xlsx',\n",
       " 'instagram (2).xlsx',\n",
       " 'instagram (3).xlsx',\n",
       " 'instagram (4).xlsx',\n",
       " 'instagram (5).xlsx',\n",
       " 'instagram (6).xlsx',\n",
       " 'instagram (7).xlsx',\n",
       " 'instagram (8).xlsx',\n",
       " 'instagram.xlsx']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_xls = [f for f in files if f[-4:] == 'xlsx']\n",
    "files_xls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for f in files_xls:\n",
    "    data = pd.read_excel(f'C:/Users/BIGNETWORK/Desktop/PFE/dataset/resaux scraper/{f}')\n",
    "    data.drop(data.index[0:0],inplace=True)\n",
    "    df = df.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"resaux_comments.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0=pd.read_excel(\"C:/Users/BIGNETWORK/Desktop/PFE/dataset/project/git-one/datasets/resaux_comments.xlsx\")\n",
    "df1=pd.read_excel(\"C:/Users/BIGNETWORK/Desktop/PFE/dataset/project/git-one/datasets/hateonly_mehdi.xlsx\")\n",
    "df2=pd.read_excel(\"C:/Users/BIGNETWORK/Desktop/PFE/dataset/project/git-one/datasets/fb_comments.xlsx\")\n",
    "df3=pd.read_excel(\"C:/Users/BIGNETWORK/Desktop/PFE/dataset/project/git-one/datasets/hainer_ranim.xlsx\")\n",
    "df4=pd.read_excel(\"C:/Users/BIGNETWORK/Desktop/PFE/dataset/project/git-one/datasets/neutre_mehdi.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df0, df1, df2, df3,df4], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>باينة كتبتوها نهار السبت حتى اليوم باش تبارتاج...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>لوكان ستنيتو حتى نهار الخميس خير و درتو بيان</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الزبون لا دخل له في البكالوريا</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>شركة.... تعجز عن وضع أجهزة تشويش بمراكز الإمتح...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>دوماج عندي لافامي في الكونت هذا كون عبرتلكم عن...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10076</th>\n",
       "      <td>رجعولنا عرض 1500نتاع 40جيغا شهرين مشي شهر هكذا...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10077</th>\n",
       "      <td>انظروا إلى التعليقات يا دجيزي بفففففففففففف</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10078</th>\n",
       "      <td>صح عيدك جيزي شوي اتهلى كونيكسيو ظعيفة وعروظ شو...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10079</th>\n",
       "      <td>عيدكم مبارك تقبل الله منا ومنكم صالح الأعمال يارب</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10080</th>\n",
       "      <td>شوف ستوري</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10081 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comments  class\n",
       "0      باينة كتبتوها نهار السبت حتى اليوم باش تبارتاج...    NaN\n",
       "1           لوكان ستنيتو حتى نهار الخميس خير و درتو بيان    NaN\n",
       "2                         الزبون لا دخل له في البكالوريا    NaN\n",
       "3      شركة.... تعجز عن وضع أجهزة تشويش بمراكز الإمتح...    NaN\n",
       "4      دوماج عندي لافامي في الكونت هذا كون عبرتلكم عن...    NaN\n",
       "...                                                  ...    ...\n",
       "10076  رجعولنا عرض 1500نتاع 40جيغا شهرين مشي شهر هكذا...    0.0\n",
       "10077        انظروا إلى التعليقات يا دجيزي بفففففففففففف    0.0\n",
       "10078  صح عيدك جيزي شوي اتهلى كونيكسيو ظعيفة وعروظ شو...    0.0\n",
       "10079  عيدكم مبارك تقبل الله منا ومنكم صالح الأعمال يارب    0.0\n",
       "10080                                          شوف ستوري    0.0\n",
       "\n",
       "[10081 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>على الكونيكسيوا الزينة تاعكم الله يذلكم</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>فعلولي عرض امتياز</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bon cnx ta3 djezzy mtsla7ch gae l  pubg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>حسام بن عبر</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>تم قطع الانترنت بشكل كامل،</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10076</th>\n",
       "      <td>@sarra.hayat mabghach yetla3li</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10077</th>\n",
       "      <td>Hahaha</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10078</th>\n",
       "      <td>مناش عايشين في بلاد واحدة !</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10079</th>\n",
       "      <td>Wallah ma tahchmou ya sraqin lmliha ki wlitou ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10080</th>\n",
       "      <td>جيزي هادي مقودا قاع في كنيكسيو</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10081 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comments  class\n",
       "0                على الكونيكسيوا الزينة تاعكم الله يذلكم   -1.0\n",
       "1                                      فعلولي عرض امتياز    NaN\n",
       "2                Bon cnx ta3 djezzy mtsla7ch gae l  pubg    NaN\n",
       "3                                            حسام بن عبر    NaN\n",
       "4                             تم قطع الانترنت بشكل كامل،    NaN\n",
       "...                                                  ...    ...\n",
       "10076                     @sarra.hayat mabghach yetla3li    0.0\n",
       "10077                                             Hahaha    0.0\n",
       "10078                        مناش عايشين في بلاد واحدة !    NaN\n",
       "10079  Wallah ma tahchmou ya sraqin lmliha ki wlitou ...    NaN\n",
       "10080                     جيزي هادي مقودا قاع في كنيكسيو   -1.0\n",
       "\n",
       "[10081 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"C:/Users/BIGNETWORK/Desktop/PFE/dataset/dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(subset = ['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments'].isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "---\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family:Georgia'> Preprocessing of Text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family:serif'> Function to Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_emoji(text):\n",
    "    return emoji.replace_emoji(text,replace=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family:serif'> removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loukan temdou alef jiga batel tqa3dou halazouuun .\n"
     ]
    }
   ],
   "source": [
    "# Load stopwords for each language\n",
    "with open('C:/Users/BIGNETWORK/Desktop/PFE/dataset/project/git-one/Algerian-Arabic-stop-words.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words_ar_dz = set([line.strip() for line in f])\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words_fr = set(stopwords.words('french'))\n",
    "# Create a custom tokenizer for Arabic words\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "# Define a function to remove stop words from a text\n",
    "def remove_stopwords(text):\n",
    "    words = tokenizer.tokenize(text)\n",
    "    words_filtered = []\n",
    "    for word in words:\n",
    "        if word not in stop_words_ar_dz and word not in stop_words_en and word not in stop_words_fr:\n",
    "            words_filtered.append(word)\n",
    "    return ' '.join(words_filtered)\n",
    "tet = \"Wlh loukan temdou alef jiga batel tqa3dou halazouuun.\"\n",
    "tet = remove_stopwords(tet)\n",
    "print(tet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family:serif'> Fucntion to remove special characters, URLs, duplicated letters, punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Apply the clean_text function to the desired column(s) in the DataFrame\\nif df['comments'].dtype == 'object':\\n    df['comments_clean'] = df['comments'].apply(clean_text)\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to perform the text cleaning operations\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation marks \n",
    "    text = re.sub(r'[@#&$]\\w+', '', text)  # Remove special characters\n",
    "    #text = re.sub(r'\\d+', '', text)  # Remove numbers \n",
    "    text = re.sub(r'(\\w)\\1+', r'\\1', text) # remove duplicated letters\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    return text\n",
    "\n",
    "'''# Apply the clean_text function to the desired column(s) in the DataFrame\n",
    "if df['comments'].dtype == 'object':\n",
    "    df['comments_clean'] = df['comments'].apply(clean_text)'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family:serif'> Function to remove mutiple sequence spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mult_spaces(text):\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family:serif'> Function to Preprocess the text by applying all above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = strip_emoji(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = clean_text(text)\n",
    "    text = remove_mult_spaces(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIGNETWORK\\AppData\\Local\\Temp\\ipykernel_4440\\2574735620.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['comments_clean'] = df['comments'].apply(preprocess)\n"
     ]
    }
   ],
   "source": [
    "if df['comments'].dtype == 'object':\n",
    "    df['comments_clean'] = df['comments'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>class</th>\n",
       "      <th>comments_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5402</th>\n",
       "      <td>نكتوها على رواحكم هذا واش نقولكم .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>نكتوها رواحكم نقولكم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>البكالوريا بقاولها يومين وتخلص مازال الحال على...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>البكالوريا بقاولها يومين وتخلص مازال الحال الا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6454</th>\n",
       "      <td>PHANTOM ND ❤️🇩🇿✌️🔥💪</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PHANTOM ND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4573</th>\n",
       "      <td>اذا ممكن مانقدرش نفليكسي من عندي لشخص آخر جربت...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>اذا مكن مانقدرش نفليكسي عندي لشخص آخر جربت 70 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8419</th>\n",
       "      <td>تم نشر هذا المنشور يوم السبت بسبب ثقل الانترنت...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>تم نشر المنشور يوم السبت بسب ثقل الانترنت زينة...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8930</th>\n",
       "      <td>Pubg mobile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pubg mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9868</th>\n",
       "      <td>قال فيه فايد</td>\n",
       "      <td>NaN</td>\n",
       "      <td>فايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7617</th>\n",
       "      <td>حتى ف الاعتذار متأخرين كالعادة</td>\n",
       "      <td>NaN</td>\n",
       "      <td>الاعتذار متأخرين كالعادة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>عندي جيزي والحمد لله، ماشكيتلك ماشكيت لواحد اوخر</td>\n",
       "      <td>NaN</td>\n",
       "      <td>عندي جيزي والحمد له ماشكيتلك ماشكيت لواحد اوخر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>أنه النفاق يا هند</td>\n",
       "      <td>1.0</td>\n",
       "      <td>أنه النفاق هند</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>جازي يسلم عليك أحمد مزبش</td>\n",
       "      <td>NaN</td>\n",
       "      <td>جازي يسلم أحمد مزبش</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7829</th>\n",
       "      <td>Mdrr allah yahdikom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mdr alah yahdikom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>EL Rey Ressan والله مكاش حتا تعويضات</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EL Rey Resan واله حتا تعويضات</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4815</th>\n",
       "      <td>Dirolna la3bt free fire illimité dok tchri dza...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dirolna la3bt fre fire ilimité tchri dzayr kml...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826</th>\n",
       "      <td>Stivane Stiv صح عيدكم 🙂</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Stivane Stiv صح عيدكم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9638</th>\n",
       "      <td>مسابقة بنكهة نقوشا 🙌😪</td>\n",
       "      <td>NaN</td>\n",
       "      <td>مسابقة بنكهة نقوشا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9826</th>\n",
       "      <td>عندي استفسار</td>\n",
       "      <td>NaN</td>\n",
       "      <td>عندي استفسار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>واااش دخلكم فالباك ؟ نتوما شركة إتصالات ولا وز...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>واش دخلكم فالباك نتوما شركة إتصالات وزارة تعلي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>@med___mer او كتبلك مدى الحياة</td>\n",
       "      <td>0.0</td>\n",
       "      <td>كتبلك مدى الحياة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9600</th>\n",
       "      <td>مضيع للوقت</td>\n",
       "      <td>NaN</td>\n",
       "      <td>مضيع لوقت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6438</th>\n",
       "      <td>حلزوني ياشكب</td>\n",
       "      <td>NaN</td>\n",
       "      <td>حلزوني ياشكب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>Tog3od avie wla kifh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tog3od avie wla kifh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>ثقيييييييييلة 🙄🙄🙄🙄</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ثقيلة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>دجيزي ما تصلحش بغرام... عندها البينڨ وهمي وكذب...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>دجيزي تصلحش بغرام عندها البينڨ وهمي وكذب كذب ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738</th>\n",
       "      <td>اسمع واش راهم يقولو راهم يخممو فيك</td>\n",
       "      <td>NaN</td>\n",
       "      <td>اسمع راهم يقولو راهم يخمو</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  class  \\\n",
       "5402                 نكتوها على رواحكم هذا واش نقولكم .    NaN   \n",
       "2675  البكالوريا بقاولها يومين وتخلص مازال الحال على...    NaN   \n",
       "6454                                PHANTOM ND ❤️🇩🇿✌️🔥💪    NaN   \n",
       "4573  اذا ممكن مانقدرش نفليكسي من عندي لشخص آخر جربت...    0.0   \n",
       "8419  تم نشر هذا المنشور يوم السبت بسبب ثقل الانترنت...    NaN   \n",
       "8930                                        Pubg mobile    NaN   \n",
       "9868                                       قال فيه فايد    NaN   \n",
       "7617                     حتى ف الاعتذار متأخرين كالعادة    NaN   \n",
       "47     عندي جيزي والحمد لله، ماشكيتلك ماشكيت لواحد اوخر    NaN   \n",
       "3252                                  أنه النفاق يا هند    1.0   \n",
       "192                            جازي يسلم عليك أحمد مزبش    NaN   \n",
       "7829                                Mdrr allah yahdikom    NaN   \n",
       "8623               EL Rey Ressan والله مكاش حتا تعويضات    0.0   \n",
       "4815  Dirolna la3bt free fire illimité dok tchri dza...    0.0   \n",
       "3826                            Stivane Stiv صح عيدكم 🙂    0.0   \n",
       "9638                              مسابقة بنكهة نقوشا 🙌😪    NaN   \n",
       "9826                                       عندي استفسار    NaN   \n",
       "8967  واااش دخلكم فالباك ؟ نتوما شركة إتصالات ولا وز...    NaN   \n",
       "3082                     @med___mer او كتبلك مدى الحياة    0.0   \n",
       "9600                                         مضيع للوقت    NaN   \n",
       "6438                                       حلزوني ياشكب    NaN   \n",
       "359                                Tog3od avie wla kifh    0.0   \n",
       "682                                  ثقيييييييييلة 🙄🙄🙄🙄    NaN   \n",
       "590   دجيزي ما تصلحش بغرام... عندها البينڨ وهمي وكذب...    NaN   \n",
       "2738                 اسمع واش راهم يقولو راهم يخممو فيك    NaN   \n",
       "\n",
       "                                         comments_clean  \n",
       "5402                              نكتوها رواحكم نقولكم   \n",
       "2675  البكالوريا بقاولها يومين وتخلص مازال الحال الا...  \n",
       "6454                                         PHANTOM ND  \n",
       "4573  اذا مكن مانقدرش نفليكسي عندي لشخص آخر جربت 70 ...  \n",
       "8419  تم نشر المنشور يوم السبت بسب ثقل الانترنت زينة...  \n",
       "8930                                        Pubg mobile  \n",
       "9868                                               فايد  \n",
       "7617                           الاعتذار متأخرين كالعادة  \n",
       "47       عندي جيزي والحمد له ماشكيتلك ماشكيت لواحد اوخر  \n",
       "3252                                     أنه النفاق هند  \n",
       "192                                 جازي يسلم أحمد مزبش  \n",
       "7829                                  Mdr alah yahdikom  \n",
       "8623                      EL Rey Resan واله حتا تعويضات  \n",
       "4815  Dirolna la3bt fre fire ilimité tchri dzayr kml...  \n",
       "3826                              Stivane Stiv صح عيدكم  \n",
       "9638                                 مسابقة بنكهة نقوشا  \n",
       "9826                                       عندي استفسار  \n",
       "8967  واش دخلكم فالباك نتوما شركة إتصالات وزارة تعلي...  \n",
       "3082                                   كتبلك مدى الحياة  \n",
       "9600                                          مضيع لوقت  \n",
       "6438                                       حلزوني ياشكب  \n",
       "359                                Tog3od avie wla kifh  \n",
       "682                                               ثقيلة  \n",
       "590   دجيزي تصلحش بغرام عندها البينڨ وهمي وكذب كذب ا...  \n",
       "2738                          اسمع راهم يقولو راهم يخمو  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments_clean'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIGNETWORK\\AppData\\Local\\Temp\\ipykernel_4440\\3493959318.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['comments_clean'].replace('', np.nan, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments_clean'].replace('', np.nan, inplace=True)\n",
    "missing_values = df[df['comments_clean'].isnull()]\n",
    "#df=df.dropna(subset = ['comments_clean'])\n",
    "len(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(subset = ['comments_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop_duplicates(subset=[\"comments_clean\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_det(texte):\n",
    "    langage = detect(texte)\n",
    "    if langage not in ['en','fr']:\n",
    "        langage = 'dz'\n",
    "    \n",
    "    return langage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping = {\n",
    "    'a': 'ا', 'kh':'خ','sh':'ش','ch':'ش','b': 'ب', 'c': 'س', 'd': 'د', 'e': 'ي', 'f': 'ف',\n",
    "    'g': 'ڨ', 'h': 'ه', 'i': 'ي', 'j': 'ج', 'k': 'ك', 'l': 'ل',\n",
    "    'm': 'م', 'n': 'ن', 'o': 'و', 'p': 'ب', 'q': 'ق', 'r': 'ر',\n",
    "    's': 'ص', 't': 'ت', 'u': 'و', 'v': 'ڥ', 'w': 'و', 'x': 'كس',\n",
    "    'y': 'ي', 'z': 'ز','9':'ق','7':'ح'\n",
    "}\n",
    "# Define the translation function\n",
    "def arabizi_to_arabic(text):\n",
    "    # Replace each Arabizi letter with its corresponding Arabic alphabet dialect letter\n",
    "    for letter, value in mapping.items():\n",
    "        text = re.sub(letter, value, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dجيززي معنديش المال باش نفليكسي ابعتلي100دج\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "arabizi_text = \"Djezzy معنديش المال باش نفليكسي ابعتلي100دج\"\n",
    "arabic_text = arabizi_to_arabic(arabizi_text)\n",
    "print(arabic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFXLMRobertaForSequenceClassification were initialized from the model checkpoint at papluca/xlm-roberta-base-language-detection.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n"
     ]
    }
   ],
   "source": [
    "# Load the language detection pipeline\n",
    "lang_detect = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n",
    "\n",
    "# Text to classify\n",
    "text = \"Pourquoi vous limitez le débit ?.\"\n",
    "\n",
    "# Run the language detection pipeline\n",
    "result = lang_detect(text)\n",
    "\n",
    "# Print the detected language\n",
    "print(result[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt\n"
     ]
    }
   ],
   "source": [
    "# Text to classify\n",
    "text = \"Vive mobilis\"\n",
    "\n",
    "# Run the language detection pipeline\n",
    "result = lang_detect(text)\n",
    "\n",
    "# Print the detected language\n",
    "print(result[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "# Utilisation de la méthode SequenceMatcher pour calculer la similarité entre deux chaînes de caractères\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "def detecter_langage(texte):\n",
    "    # Détection de la langue avec langdetect\n",
    "    langage = detect(texte)\n",
    "    \n",
    "   #  Si le texte est similaire à l'anglais, au français ou à l'arabe, il est classé comme dialecte algérien\n",
    "    if langage in ['en', 'fr', 'ar']:\n",
    "        similarites = {\n",
    "            'en': similar(texte, 'english'),\n",
    "            'fr': similar(texte, 'français'),\n",
    "            'ar': similar(texte, 'العربية')\n",
    "        }\n",
    "        # Le texte est classé comme dialecte algérien s'il est plus similaire à une de ces langues qu'à l'autre\n",
    "        if max(similarites.values()) == similarites[langage] or langage =='so':\n",
    "            langage = 'dz'\n",
    "    \n",
    "    return langage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Kanet 3endi beseh réseau fiha mekhsous bezaf f la willaya de Tizi ouzou apart le centre ville makanch réseau\"\n",
    "print(detecter_langage(txt))\n",
    "  #  df['len'] = df['comments_nett'].apply(detecter_langage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIGNETWORK\\AppData\\Local\\Temp\\ipykernel_19160\\2026574333.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.at[index, 'language'] = result[0]['label']\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (818 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    text = str(row['comments_clean'])\n",
    "    if text:\n",
    "        try:\n",
    "            result = lang_detect(text)\n",
    "            df.at[index, 'language'] = result[0]['label']\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64a6b5e2244e422069341277cd1317ebf2b03c7f83a07567628f5bb1b1823540"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
